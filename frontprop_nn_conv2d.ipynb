{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_22180\\1437536937.py:12: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import scipy\n",
    "from scipy.spatial import distance\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn import preprocessing\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from tqdm.autonotebook import tqdm\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e204b63f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.nn.Conv2d(in_channels, out_channels, kernel_size, stride=1, padding=0, dilation=1, groups=1, bias=True, padding_mode='zeros', device=None, dtype=None)\n",
    "\n",
    "# TODO: normalise input and weights + use gaussian initialisation\n",
    "\n",
    "T_DECAY_DEFAULT = 0.0005\n",
    "W_BOOST_DEFAULT = 0.02\n",
    "\n",
    "class FpConv2d(nn.Conv2d):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, stride=1, padding=0, dilation=1, groups=1, bias=False, padding_mode='zeros', t_decay=T_DECAY_DEFAULT, w_boost=W_BOOST_DEFAULT, device=None, dtype=None):\n",
    "        if bias:\n",
    "            raise Exception(\"Bias not supported for FrontPropConv2d\")\n",
    "        if dilation != 1:\n",
    "            raise Exception(\"Dilation not supported for FrontPropConv2d\")\n",
    "        if groups != 1:\n",
    "            raise Exception(\"Groups not supported for FrontPropConv2d\")\n",
    "        if kernel_size[0] != kernel_size[1]:\n",
    "            raise Exception(\"Non-square kernel not supported for FrontPropConv2d\")\n",
    "        \n",
    "        super(FpConv2d, self).__init__(in_channels, out_channels, kernel_size, stride, padding, dilation, groups, bias, padding_mode, device, dtype)\n",
    "\n",
    "        # learning is through front propagation only\n",
    "        self.weight.requires_grad = False\n",
    "\n",
    "        # init using Gaussian\n",
    "        nn.init.normal_(self.weight, mean=0, std=1)\n",
    "\n",
    "        self.kernel_size = kernel_size[0]\n",
    "\n",
    "        # hyper params\n",
    "        self.t_decay = t_decay\n",
    "        self.w_boost = w_boost\n",
    "\n",
    "        self.frozen = False\n",
    "\n",
    "        assert self.t_decay > 0        \n",
    "        assert self.w_boost > 0\n",
    "\n",
    "        # see lazy_init_thresholds()\n",
    "        self.t = None\n",
    "    \n",
    "\n",
    "    def lazy_init_thresholds(self, out_h, out_w):\n",
    "        if self.t is None:\n",
    "            self.t = torch.ones(self.out_channels, out_h, out_w, device=self.device, dtype=self.weight.dtype)\n",
    "\n",
    "\n",
    "    def __normalise_unitary(self, data, dim):\n",
    "        return data / torch.norm(data, dim=dim, keepdim=True)\n",
    "    \n",
    "\n",
    "    def __get_weights_boost(self, kernel_idx, data_tensor):\n",
    "        # FIXME: can i use this addition, or do i need to use the angle?\n",
    "\n",
    "        kernel_weights = self.weight[kernel_idx]\n",
    "\n",
    "        assert data_tensor.shape == kernel_weights.shape\n",
    "\n",
    "        w_boost = self.w_boost * (data_tensor - kernel_weights)\n",
    "\n",
    "        return w_boost\n",
    "    \n",
    "\n",
    "    def get_input_patch(self, input, sample_idx, out_h, out_w):\n",
    "        in_h = out_h * self.stride - self.padding\n",
    "        in_w = out_w * self.stride - self.padding\n",
    "        return input[sample_idx, :, \n",
    "                     in_h : in_h + self.kernel_size, \n",
    "                     in_w : in_w + self.kernel_size]\n",
    "    \n",
    "\n",
    "    def forward(self, input):\n",
    "\n",
    "        assert input.shape[1] == self.in_channels\n",
    "        \n",
    "        self.output = F.conv2d(input, self.weights, self.biases, self.stride, self.padding, self.dilation, self.groups)\n",
    "\n",
    "        assert self.output.shape == (input.shape[0], self.out_channels, input.shape[-2], input.shape[-1])\n",
    "\n",
    "        self.lazy_init_thresholds(self.output.shape[-2], self.output.shape[-1])\n",
    "\n",
    "        assert self.t.shape == self.output.shape\n",
    "\n",
    "        # ReLU-like non-linear transformation via cutoff threshold\n",
    "        self.output = torch.where(self.output >= self.t, self.output - self.t, torch.zeros_like(self.output))\n",
    "\n",
    "        \n",
    "        # Learning happens below:\n",
    "        #\n",
    "        #   For each location where the threshold was exceeded, update the weights.\n",
    "        #   The weights are updated by a small amount towards the input data tensor\n",
    "        #   and thresholds are set to the new value where the threshold was exceeded.\n",
    "        #   Thresholds are also decayed by a small amount on each pass.\n",
    "        #\n",
    "        #   Convolution kernels' weights are updated in random order (of samples and locations).\n",
    "        #\n",
    "        #   The output is equal to the activation above the threshold (or zero if below threshold).\n",
    "        #\n",
    "        #   ---\n",
    "        #   TODO: Should we output the absolute value or only the diff above threshold ?\n",
    "        #   (note the threshold changes on each pass)\n",
    "        #\n",
    "        #   FIXME: all inputs should be first normalised - this is tricky\n",
    "        #   TODO: Optimise this\n",
    "        #   * Try removing loops\n",
    "        #   * maybe use torch.sparse_coo_tensor() to save memory\n",
    "        if not self.frozen:\n",
    "            \n",
    "            # Update weights:\n",
    "\n",
    "            excitations_idxs = torch.nonzero(self.output)\n",
    "            # shuffle indices randomly\n",
    "            excitations_idxs = excitations_idxs[torch.randperm(excitations_idxs.shape[0])]\n",
    "            # iterate locations where threshold exceeded and shift weights closer to the input\n",
    "            for sample_idx, kernel_idx, h, w in excitations_idxs:\n",
    "                data_tensor = self.get_input_patch(input, sample_idx, h, w)\n",
    "                assert data_tensor.shape == (self.in_channels, self.kernel_size, self.kernel_size)\n",
    "                data_tensor = self.__normalise_unitary(data_tensor, dim=(1,2))\n",
    "                assert data_tensor.shape == (self.in_channels, self.kernel_size, self.kernel_size)\n",
    "                # update weights\n",
    "                self.weight[kernel_idx] += self.__get_weights_boost(kernel_idx, data_tensor)\n",
    "                # normalise weights\n",
    "                self.weight[kernel_idx] = self.__normalise_unitary(self.weight[kernel_idx], dim=(1,2))\n",
    "                assert self.weight[kernel_idx].shape == (self.in_channels, self.kernel_size, self.kernel_size)\n",
    "\n",
    "            # Update thresholds:\n",
    "                \n",
    "            self.binary_excitations = self.output >= 0\n",
    "            # where threshold exceeded set it to the new value\n",
    "            self.t = self.binary_excitations * self.output + ~self.binary_excitations * self.t\n",
    "            # decay all thresholds\n",
    "            self.t = self.t * (1.0 - self.t_decay)\n",
    "\n",
    "\n",
    "        return self.output\n",
    "\n",
    "    def backward(self, grad_output):\n",
    "        raise Exception(\"Backward pass not implemented for FrontPropConv2d\")\n",
    "\n",
    "\n",
    "    def freeze(self):\n",
    "        self.frozen = True\n",
    "            \n",
    "\n",
    "    def unfreeze(self):\n",
    "        self.frozen = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e668ed31",
   "metadata": {},
   "outputs": [],
   "source": [
    "in_ch = 3\n",
    "out_ch = 5\n",
    "kernel_size = 2\n",
    "stride = 1\n",
    "\n",
    "conv2d = nn.Conv2d(in_ch, out_ch, kernel_size=kernel_size, stride=stride)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "28558ae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 10\n",
    "img_w = 16\n",
    "img_h = 16\n",
    "\n",
    "# 3 channels, 16x16 image\n",
    "x = torch.rand((batch_size, in_ch, img_w, img_h))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "1fea2306",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0,  0,  0,  0],\n",
       "        [ 0,  0,  0,  1],\n",
       "        [ 0,  0,  0,  2],\n",
       "        ...,\n",
       "        [ 9,  2, 15, 13],\n",
       "        [ 9,  2, 15, 14],\n",
       "        [ 9,  2, 15, 15]])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.nonzero(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "275d091a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ True,  True,  True,  True],\n",
       "        [ True,  True,  True, False],\n",
       "        [ True,  True,  True, False],\n",
       "        ...,\n",
       "        [False, False, False, False],\n",
       "        [False, False, False, False],\n",
       "        [False, False, False, False]])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "~(torch.nonzero(x) > 0) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "52f95422",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 3, 2, 2])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# out_ch x in_ch x kernel_size x kernel_size\n",
    "conv2d.weight.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ba46bf1c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# out_ch\n",
    "conv2d.bias.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e4f01608",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 5, 15, 15])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# batch_size x out_ch x img_w x img_h\n",
    "out = conv2d(x)\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "dc52ad0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 3, 15, 15, 2, 2])\n",
      "15 15\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 3, 30, 30])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "patches = x.unfold(2, kernel_size, stride).unfold(3, kernel_size, stride)\n",
    "print(patches.shape)\n",
    "\n",
    "# Get output shape\n",
    "out_height, out_width = out.shape[-2:]\n",
    "print(out_height, out_width)\n",
    "\n",
    "# Reshape and permute patches to get exploded output\n",
    "exploded_output = patches.contiguous().view(batch_size, in_ch, out_height * kernel_size, out_width * kernel_size)\n",
    "exploded_output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da6bf2e1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9e392e6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://github.com/pytorch/vision/zipball/v0.10.0\" to C:\\Users\\karol/.cache\\torch\\hub\\v0.10.0.zip\n",
      "Downloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to C:\\Users\\karol/.cache\\torch\\hub\\checkpoints\\resnet18-f37072fd.pth\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 44.7M/44.7M [00:04<00:00, 9.98MB/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ResNet(\n",
       "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu): ReLU(inplace=True)\n",
       "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  (layer1): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  (fc): Linear(in_features=512, out_features=1000, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = torch.hub.load('pytorch/vision:v0.10.0', 'resnet18', pretrained=True)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41061f90",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
